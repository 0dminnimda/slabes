
#!/usr/bin/env python
# @generated by pegen from slabes/slabes.peg

import sys
import tokenize
import itertools
import typing

from typing import Any, Optional

from pegen.parser import memoize, memoize_left_rec, logger
from . import ast_nodes as ast
from .parser_base import ParserBase as Parser, parser_main
# Keywords and soft keywords are listed at the end of the parser definition.
class SlabesParser(Parser):

    @memoize
    def start(self) -> Optional[ast . Module]:
        # start: statements $
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.statements())
            and
            (self.expect('ENDMARKER'))
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . Module ( a [0] , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def statements(self) -> Optional[tuple [list [ast . Statement]]]:
        # statements: statement_group+
        mark = self._mark()
        if (
            (a := self._loop1_1())
        ):
            return ( list ( itertools . chain . from_iterable ( i [0] for i in a ) ) , );
        self._reset(mark)
        return None;

    @memoize
    def statement_group(self) -> Optional[tuple [list [ast . Statement]]]:
        # statement_group: ((','+).statement+)? ','* '.'
        mark = self._mark()
        if (
            (stmts := self._gather_2(),)
            and
            (self._loop0_4(),)
            and
            (self.expect('.'))
        ):
            return ( [] if stmts is None else stmts , );
        self._reset(mark)
        return None;

    @memoize
    def statement(self) -> Optional[ast . Statement]:
        # statement: &UNTIL ~ until_stmt | &CHECK ~ check_stmt | function_definition | declaration | expr
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        cut = False
        if (
            (self.positive_lookahead(self.UNTIL, ))
            and
            (cut := True)
            and
            (until_stmt := self.until_stmt())
        ):
            return until_stmt;
        self._reset(mark)
        if cut:
            return None;
        cut = False
        if (
            (self.positive_lookahead(self.CHECK, ))
            and
            (cut := True)
            and
            (check_stmt := self.check_stmt())
        ):
            return check_stmt;
        self._reset(mark)
        if cut:
            return None;
        if (
            (function_definition := self.function_definition())
        ):
            return function_definition;
        self._reset(mark)
        if (
            (declaration := self.declaration())
        ):
            return declaration;
        self._reset(mark)
        if (
            (expr := self.expr())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . SingleExpression ( expr , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def function_definition(self) -> Optional[ast . Function]:
        # function_definition: number_type identifier (','.argument+)? ','? BEGIN statement_group END | recover_function_definition
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (ret := self.number_type())
            and
            (name := self.identifier())
            and
            (args := self._gather_5(),)
            and
            (self.expect(','),)
            and
            (self.BEGIN())
            and
            (b := self.statement_group())
            and
            (self.END())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . Function ( ret , name . value , [] if args is None else args , b [0] , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (recover_function_definition := self.recover_function_definition())
        ):
            return recover_function_definition;
        self._reset(mark)
        return None;

    @memoize
    def recover_function_definition(self) -> Optional[Any]:
        # recover_function_definition: word word (','.argument+)? ','? BEGIN statement_group END
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (ret := self.word())
            and
            (name := self.word())
            and
            (args := self._gather_7(),)
            and
            (self.expect(','),)
            and
            (self.BEGIN())
            and
            (b := self.statement_group())
            and
            (self.END())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . Function ( self . make_number_type ( ret , ** self . locs ( ret ) ) , self . make_name ( name , ** self . locs ( name ) ) . value , [] if args is None else args , b [0] , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def argument(self) -> Optional[ast . Argument]:
        # argument: number_type identifier | recover_argument
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (tp := self.number_type())
            and
            (id := self.identifier())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . Argument ( tp , id , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (recover_argument := self.recover_argument())
        ):
            return recover_argument;
        self._reset(mark)
        return None;

    @memoize
    def recover_argument(self) -> Optional[Any]:
        # recover_argument: word word
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (tp := self.word())
            and
            (id := self.word())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . Argument ( self . make_number_type ( tp , ** self . locs ( tp ) ) , self . make_name ( id , ** self . locs ( id ) ) , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def until_stmt(self) -> Optional[ast . Until]:
        # until_stmt: UNTIL expr DO statement_group
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (self.UNTIL())
            and
            (expr := self.expr())
            and
            (self.DO())
            and
            (statement_group := self.statement_group())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . Until ( expr , statement_group [0] , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def check_stmt(self) -> Optional[ast . Check]:
        # check_stmt: CHECK expr DO statement_group
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (self.CHECK())
            and
            (expr := self.expr())
            and
            (self.DO())
            and
            (statement_group := self.statement_group())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . Check ( expr , statement_group [0] , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def declaration(self) -> Optional[ast . NumberDeclaration]:
        # declaration: &FIELD ~ array_declaration | number_declaration
        mark = self._mark()
        cut = False
        if (
            (self.positive_lookahead(self.FIELD, ))
            and
            (cut := True)
            and
            (array_declaration := self.array_declaration())
        ):
            return array_declaration;
        self._reset(mark)
        if cut:
            return None;
        if (
            (number_declaration := self.number_declaration())
        ):
            return number_declaration;
        self._reset(mark)
        return None;

    @memoize
    def array_declaration(self) -> Optional[ast . ArrayDeclaration]:
        # array_declaration: FIELD number_type number_type identifier+ '<<' expr | recover_array_declaration
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (self.FIELD())
            and
            (elem_t := self.number_type())
            and
            (size_t := self.number_type())
            and
            (names := self._loop1_9())
            and
            (self.expect('<<'))
            and
            (val := self.expr())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_array_declaration ( elem_t , size_t , names , val , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (recover_array_declaration := self.recover_array_declaration())
        ):
            return recover_array_declaration;
        self._reset(mark)
        return None;

    @memoize
    def recover_array_declaration(self) -> Optional[ast . ArrayDeclaration]:
        # recover_array_declaration: FIELD word word word+ '<<' expr
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (self.FIELD())
            and
            (elem_t := self.word())
            and
            (size_t := self.word())
            and
            (names := self._loop1_10())
            and
            (self.expect('<<'))
            and
            (val := self.expr())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_array_declaration ( self . make_number_type ( elem_t , ** self . locs ( elem_t ) ) , self . make_number_type ( size_t , ** self . locs ( size_t ) ) , [self . make_name ( name , ** self . locs ( name ) ) for name in names] , val , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def number_declaration(self) -> Optional[ast . NumberDeclaration]:
        # number_declaration: number_type identifier+ '<<' expr | recover_number_declaration
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (type := self.number_type())
            and
            (names := self._loop1_11())
            and
            (self.expect('<<'))
            and
            (val := self.expr())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_number_declaration ( type , names , val , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (recover_number_declaration := self.recover_number_declaration())
        ):
            return recover_number_declaration;
        self._reset(mark)
        return None;

    @memoize
    def recover_number_declaration(self) -> Optional[ast . NumberDeclaration]:
        # recover_number_declaration: word word+ '<<' expr
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (type := self.word())
            and
            (names := self._loop1_12())
            and
            (self.expect('<<'))
            and
            (val := self.expr())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_number_declaration ( self . make_number_type ( type , ** self . locs ( type ) ) , [self . make_name ( name , ** self . locs ( name ) ) for name in names] , val , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def expr(self) -> Optional[ast . Expression]:
        # expr: assignment | invalid_expr
        mark = self._mark()
        if (
            (assignment := self.assignment())
        ):
            return assignment;
        self._reset(mark)
        if (
            self.call_invalid_rules
            and
            (self.invalid_expr())
        ):
            return None  # pragma: no cover;
        self._reset(mark)
        return None;

    @memoize
    def invalid_expr(self) -> Optional[Any]:
        # invalid_expr: declaration
        mark = self._mark()
        if (
            (a := self.declaration())
        ):
            return self . report_syntax_error_at ( "expected expression, got declaration." " Did you mean to use '.' before/after this declaration?" , a , fatal = True , );
        self._reset(mark)
        return None;

    @memoize
    def assignment(self) -> Optional[Any]:
        # assignment: comparison ((('<<' | '>>') comparison))+ | comparison
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (start := self.comparison())
            and
            (rest := self._loop1_13())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_assignment ( start , rest , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (comparison := self.comparison())
        ):
            return comparison;
        self._reset(mark)
        return None;

    @memoize
    def comparison(self) -> Optional[Any]:
        # comparison: sum
        mark = self._mark()
        if (
            (sum := self.sum())
        ):
            return sum;
        self._reset(mark)
        return None;

    @memoize_left_rec
    def sum(self) -> Optional[Any]:
        # sum: sum '+' term | sum '-' term | term
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.sum())
            and
            (self.expect('+'))
            and
            (b := self.term())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . BinaryOperation ( a , ast . BinOp . ADD , b , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (a := self.sum())
            and
            (self.expect('-'))
            and
            (b := self.term())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . BinaryOperation ( a , ast . BinOp . SUB , b , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (term := self.term())
        ):
            return term;
        self._reset(mark)
        return None;

    @memoize_left_rec
    def term(self) -> Optional[Any]:
        # term: term '\\' factor | term '/' factor | factor
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.term())
            and
            (self.expect('\\'))
            and
            (b := self.factor())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . BinaryOperation ( a , ast . BinOp . MUL , b , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (a := self.term())
            and
            (self.expect('/'))
            and
            (b := self.factor())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return ast . BinaryOperation ( a , ast . BinOp . DIV , b , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (factor := self.factor())
        ):
            return factor;
        self._reset(mark)
        return None;

    @memoize
    def factor(self) -> Optional[Any]:
        # factor: primary
        mark = self._mark()
        if (
            (primary := self.primary())
        ):
            return primary;
        self._reset(mark)
        return None;

    @memoize_left_rec
    def primary(self) -> Optional[Any]:
        # primary: subscript | call | atom
        mark = self._mark()
        if (
            (subscript := self.subscript())
        ):
            return subscript;
        self._reset(mark)
        if (
            (call := self.call())
        ):
            return call;
        self._reset(mark)
        if (
            (atom := self.atom())
        ):
            return atom;
        self._reset(mark)
        return None;

    @logger
    def subscript(self) -> Optional[ast . Subscript]:
        # subscript: primary '[' expr* ']' | recover_subscript
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.primary())
            and
            (self.expect('['))
            and
            (exprs := self._loop0_14(),)
            and
            (self.expect(']'))
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_subscript ( a , exprs , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (recover_subscript := self.recover_subscript())
        ):
            return recover_subscript;
        self._reset(mark)
        return None;

    @memoize
    def recover_subscript(self) -> Optional[ast . Subscript]:
        # recover_subscript: word '[' expr* ']'
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.word())
            and
            (self.expect('['))
            and
            (exprs := self._loop0_15(),)
            and
            (self.expect(']'))
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_subscript ( self . make_name ( a , ** self . locs ( a ) ) , exprs , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @logger
    def call(self) -> Optional[ast . Call]:
        # call: primary '(' expr* ')' | recover_call
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.primary())
            and
            (self.expect('('))
            and
            (exprs := self._loop0_16(),)
            and
            (self.expect(')'))
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_call ( a , exprs , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        if (
            (recover_call := self.recover_call())
        ):
            return recover_call;
        self._reset(mark)
        return None;

    @memoize
    def recover_call(self) -> Optional[ast . Call]:
        # recover_call: word '(' expr* ')'
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.word())
            and
            (self.expect('('))
            and
            (exprs := self._loop0_17(),)
            and
            (self.expect(')'))
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_call ( self . make_name ( a , ** self . locs ( a ) ) , exprs , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def atom(self) -> Optional[Any]:
        # atom: identifier | signed_number | robot_operation | &'(' group | recover_atom
        mark = self._mark()
        if (
            (identifier := self.identifier())
        ):
            return identifier;
        self._reset(mark)
        if (
            (signed_number := self.signed_number())
        ):
            return signed_number;
        self._reset(mark)
        if (
            (robot_operation := self.robot_operation())
        ):
            return robot_operation;
        self._reset(mark)
        if (
            (self.positive_lookahead(self.expect, '('))
            and
            (group := self.group())
        ):
            return group;
        self._reset(mark)
        if (
            (recover_atom := self.recover_atom())
        ):
            return recover_atom;
        self._reset(mark)
        return None;

    @memoize
    def recover_atom(self) -> Optional[Any]:
        # recover_atom: word
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.word())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_name ( a , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def group(self) -> Optional[Any]:
        # group: '(' comparison ')'
        mark = self._mark()
        if (
            (self.expect('('))
            and
            (a := self.comparison())
            and
            (self.expect(')'))
        ):
            return a;
        self._reset(mark)
        return None;

    @memoize
    def robot_operation(self) -> Optional[ast . RobotOperation]:
        # robot_operation: robot_keyword
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (op := self.robot_keyword())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_robot_op ( op , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def robot_keyword(self) -> Optional[Any]:
        # robot_keyword: GO | RL | RR | SONAR | COMPASS
        mark = self._mark()
        if (
            (GO := self.GO())
        ):
            return GO;
        self._reset(mark)
        if (
            (RL := self.RL())
        ):
            return RL;
        self._reset(mark)
        if (
            (RR := self.RR())
        ):
            return RR;
        self._reset(mark)
        if (
            (SONAR := self.SONAR())
        ):
            return SONAR;
        self._reset(mark)
        if (
            (COMPASS := self.COMPASS())
        ):
            return COMPASS;
        self._reset(mark)
        return None;

    @memoize
    def signed_number(self) -> Optional[ast . NumericLiteral]:
        # signed_number: sign? NUMBER
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (sign := self.sign(),)
            and
            (num := self.number())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_number ( num , sign , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def sign(self) -> Optional[Any]:
        # sign: '+' | '-'
        mark = self._mark()
        if (
            (literal := self.expect('+'))
        ):
            return literal;
        self._reset(mark)
        if (
            (literal := self.expect('-'))
        ):
            return literal;
        self._reset(mark)
        return None;

    @memoize
    def number_type(self) -> Optional[ast . NumbeTypeRef]:
        # number_type: number_type_raw
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.number_type_raw())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_number_type ( a , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def number_type_raw(self) -> Optional[Any]:
        # number_type_raw: TINY | SMALL | NORMAL | BIG
        mark = self._mark()
        if (
            (TINY := self.TINY())
        ):
            return TINY;
        self._reset(mark)
        if (
            (SMALL := self.SMALL())
        ):
            return SMALL;
        self._reset(mark)
        if (
            (NORMAL := self.NORMAL())
        ):
            return NORMAL;
        self._reset(mark)
        if (
            (BIG := self.BIG())
        ):
            return BIG;
        self._reset(mark)
        return None;

    @memoize
    def identifier(self) -> Optional[ast . Name]:
        # identifier: NAME
        mark = self._mark()
        tok = self._tokenizer.peek()
        start_lineno, start_col_offset = tok.start
        if (
            (a := self.name())
        ):
            tok = self._tokenizer.get_last_non_whitespace_token()
            end_lineno, end_col_offset = tok.end
            return self . make_name ( a , lineno=start_lineno, col_offset=start_col_offset, end_lineno=end_lineno, end_col_offset=end_col_offset );
        self._reset(mark)
        return None;

    @memoize
    def word(self) -> Optional[Any]:
        # word: NAME | keyword
        mark = self._mark()
        if (
            (name := self.name())
        ):
            return name;
        self._reset(mark)
        if (
            (keyword := self.keyword())
        ):
            return keyword;
        self._reset(mark)
        return None;

    @memoize
    def keyword(self) -> Optional[Any]:
        # keyword: number_type_raw | FIELD | BEGIN | END | UNTIL | DO | CHECK | robot_keyword
        mark = self._mark()
        if (
            (number_type_raw := self.number_type_raw())
        ):
            return number_type_raw;
        self._reset(mark)
        if (
            (FIELD := self.FIELD())
        ):
            return FIELD;
        self._reset(mark)
        if (
            (BEGIN := self.BEGIN())
        ):
            return BEGIN;
        self._reset(mark)
        if (
            (END := self.END())
        ):
            return END;
        self._reset(mark)
        if (
            (UNTIL := self.UNTIL())
        ):
            return UNTIL;
        self._reset(mark)
        if (
            (DO := self.DO())
        ):
            return DO;
        self._reset(mark)
        if (
            (CHECK := self.CHECK())
        ):
            return CHECK;
        self._reset(mark)
        if (
            (robot_keyword := self.robot_keyword())
        ):
            return robot_keyword;
        self._reset(mark)
        return None;

    @memoize
    def _loop1_1(self) -> Optional[Any]:
        # _loop1_1: statement_group
        mark = self._mark()
        children = []
        while (
            (statement_group := self.statement_group())
        ):
            children.append(statement_group)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop0_3(self) -> Optional[Any]:
        # _loop0_3: (','+) statement
        mark = self._mark()
        children = []
        while (
            (self._loop1_18())
            and
            (elem := self.statement())
        ):
            children.append(elem)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _gather_2(self) -> Optional[Any]:
        # _gather_2: statement _loop0_3
        mark = self._mark()
        if (
            (elem := self.statement())
            is not None
            and
            (seq := self._loop0_3())
            is not None
        ):
            return [elem] + seq;
        self._reset(mark)
        return None;

    @memoize
    def _loop0_4(self) -> Optional[Any]:
        # _loop0_4: ','
        mark = self._mark()
        children = []
        while (
            (literal := self.expect(','))
        ):
            children.append(literal)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop0_6(self) -> Optional[Any]:
        # _loop0_6: ',' argument
        mark = self._mark()
        children = []
        while (
            (self.expect(','))
            and
            (elem := self.argument())
        ):
            children.append(elem)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _gather_5(self) -> Optional[Any]:
        # _gather_5: argument _loop0_6
        mark = self._mark()
        if (
            (elem := self.argument())
            is not None
            and
            (seq := self._loop0_6())
            is not None
        ):
            return [elem] + seq;
        self._reset(mark)
        return None;

    @memoize
    def _loop0_8(self) -> Optional[Any]:
        # _loop0_8: ',' argument
        mark = self._mark()
        children = []
        while (
            (self.expect(','))
            and
            (elem := self.argument())
        ):
            children.append(elem)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _gather_7(self) -> Optional[Any]:
        # _gather_7: argument _loop0_8
        mark = self._mark()
        if (
            (elem := self.argument())
            is not None
            and
            (seq := self._loop0_8())
            is not None
        ):
            return [elem] + seq;
        self._reset(mark)
        return None;

    @memoize
    def _loop1_9(self) -> Optional[Any]:
        # _loop1_9: identifier
        mark = self._mark()
        children = []
        while (
            (identifier := self.identifier())
        ):
            children.append(identifier)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop1_10(self) -> Optional[Any]:
        # _loop1_10: word
        mark = self._mark()
        children = []
        while (
            (word := self.word())
        ):
            children.append(word)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop1_11(self) -> Optional[Any]:
        # _loop1_11: identifier
        mark = self._mark()
        children = []
        while (
            (identifier := self.identifier())
        ):
            children.append(identifier)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop1_12(self) -> Optional[Any]:
        # _loop1_12: word
        mark = self._mark()
        children = []
        while (
            (word := self.word())
        ):
            children.append(word)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop1_13(self) -> Optional[Any]:
        # _loop1_13: (('<<' | '>>') comparison)
        mark = self._mark()
        children = []
        while (
            (_tmp_19 := self._tmp_19())
        ):
            children.append(_tmp_19)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop0_14(self) -> Optional[Any]:
        # _loop0_14: expr
        mark = self._mark()
        children = []
        while (
            (expr := self.expr())
        ):
            children.append(expr)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop0_15(self) -> Optional[Any]:
        # _loop0_15: expr
        mark = self._mark()
        children = []
        while (
            (expr := self.expr())
        ):
            children.append(expr)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop0_16(self) -> Optional[Any]:
        # _loop0_16: expr
        mark = self._mark()
        children = []
        while (
            (expr := self.expr())
        ):
            children.append(expr)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop0_17(self) -> Optional[Any]:
        # _loop0_17: expr
        mark = self._mark()
        children = []
        while (
            (expr := self.expr())
        ):
            children.append(expr)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _loop1_18(self) -> Optional[Any]:
        # _loop1_18: ','
        mark = self._mark()
        children = []
        while (
            (literal := self.expect(','))
        ):
            children.append(literal)
            mark = self._mark()
        self._reset(mark)
        return children;

    @memoize
    def _tmp_19(self) -> Optional[Any]:
        # _tmp_19: ('<<' | '>>') comparison
        mark = self._mark()
        if (
            (_tmp_20 := self._tmp_20())
            and
            (comparison := self.comparison())
        ):
            return [_tmp_20, comparison];
        self._reset(mark)
        return None;

    @memoize
    def _tmp_20(self) -> Optional[Any]:
        # _tmp_20: '<<' | '>>'
        mark = self._mark()
        if (
            (literal := self.expect('<<'))
        ):
            return literal;
        self._reset(mark)
        if (
            (literal := self.expect('>>'))
        ):
            return literal;
        self._reset(mark)
        return None;

    KEYWORDS = ()
    SOFT_KEYWORDS = ()

if __name__ == '__main__':
    parser_main(SlabesParser)
